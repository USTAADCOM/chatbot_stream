{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgXN3qenG13cMQaUZkuNe1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USTAADCOM/chatbot_stream/blob/main/ChatBot_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Libraries Required**"
      ],
      "metadata": {
        "id": "lbeXmUYmFEjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install text-generation\n",
        "! pip install gradio"
      ],
      "metadata": {
        "id": "2KvZp-ZtE8_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ChatBot Stream**"
      ],
      "metadata": {
        "id": "ZjA1hO2YE34e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9xumQPNEu2Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "from text_generation import Client, InferenceAPIClient\n",
        "\n",
        "def get_client(model: str):\n",
        "    return InferenceAPIClient(model, token=os.getenv(\"HF_TOKEN\", None))\n",
        "\n",
        "def get_usernames(model: str):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        (str, str, str, str): pre-prompt, username, bot name, separator\n",
        "    \"\"\"\n",
        "    if model in (\"OpenAssistant/oasst-sft-1-pythia-12b\", \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"):\n",
        "        return \"\", \"<|prompter|>\", \"<|assistant|>\", \"<|endoftext|>\"\n",
        "def predict(\n",
        "    model: str,\n",
        "    inputs: str,\n",
        "    typical_p: float,\n",
        "    watermark: bool,\n",
        "    chatbot,\n",
        "    history):\n",
        "    client = get_client(model)\n",
        "    preprompt, user_name, assistant_name, sep = get_usernames(model)\n",
        "    history.append(inputs)\n",
        "    past = []\n",
        "    for data in chatbot:\n",
        "        user_data, model_data = data\n",
        "\n",
        "        if not user_data.startswith(user_name):\n",
        "            user_data = user_name + user_data\n",
        "        if not model_data.startswith(sep + assistant_name):\n",
        "            model_data = sep + assistant_name + model_data\n",
        "        past.append(user_data + model_data.rstrip() + sep)\n",
        "    if not inputs.startswith(user_name):\n",
        "        inputs = user_name + inputs\n",
        "    total_inputs = preprompt + \"\".join(past) + inputs + sep + assistant_name.rstrip()\n",
        "    partial_words = \"\"\n",
        "    if model in (\"OpenAssistant/oasst-sft-1-pythia-12b\", \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"):\n",
        "        iterator = client.generate_stream(\n",
        "            total_inputs,\n",
        "            typical_p=typical_p,\n",
        "            truncate=1000,\n",
        "            watermark=watermark,\n",
        "            max_new_tokens=500,\n",
        "        )\n",
        "    for i, response in enumerate(iterator):\n",
        "        if response.token.special:\n",
        "            continue\n",
        "        partial_words = partial_words + response.token.text\n",
        "        if partial_words.endswith(user_name.rstrip()):\n",
        "            partial_words = partial_words.rstrip(user_name.rstrip())\n",
        "        if partial_words.endswith(assistant_name.rstrip()):\n",
        "            partial_words = partial_words.rstrip(assistant_name.rstrip())\n",
        "        if i == 0:\n",
        "            history.append(\" \" + partial_words)\n",
        "        elif response.token.text not in user_name:\n",
        "            history[-1] = partial_words\n",
        "        chat = [\n",
        "            (history[i].strip(), history[i + 1].strip())\n",
        "            for i in range(0, len(history) - 1, 2)\n",
        "        ]\n",
        "        yield chat, history\n",
        "\n",
        "def reset_textbox():\n",
        "    return gr.update(value=\"\")\n",
        "\n",
        "\n",
        "def radio_on_change(\n",
        "    value: str,\n",
        "    disclaimer,\n",
        "    typical_p,\n",
        "    watermark,\n",
        "):\n",
        "    if value in (\"OpenAssistant/oasst-sft-1-pythia-12b\", \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"):\n",
        "        typical_p = typical_p.update(value=0.2, visible=True)\n",
        "        watermark = watermark.update(False)\n",
        "    return (\n",
        "        disclaimer,\n",
        "        typical_p,\n",
        "        watermark,\n",
        "    )\n",
        "with gr.Blocks(\n",
        "    css=\"\"\"#col_container {margin-left: auto; margin-right: auto;}\n",
        "                #chatbot {height: 520px; overflow: auto;}\"\"\") as demo:\n",
        "    with gr.Column(elem_id=\"col_container\"):\n",
        "        model = gr.Radio(\n",
        "            value=\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",\n",
        "            choices=[\n",
        "                \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",\n",
        "                \"OpenAssistant/oasst-sft-1-pythia-12b\"],\n",
        "            label = \"Model\",\n",
        "            interactive=True)\n",
        "        chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
        "        inputs = gr.Textbox(\n",
        "            placeholder = \"Hi there!\", label = \"Type an input and press Enter\")\n",
        "        state = gr.State([])\n",
        "        b1 = gr.Button()\n",
        "        with gr.Accordion(\"Parameters\", open = False):\n",
        "            typical_p = gr.Slider(\n",
        "                minimum = -0,\n",
        "                maximum = 1.0,\n",
        "                value = 0.2,\n",
        "                step = 0.05,\n",
        "                interactive = True,\n",
        "                label=\"Typical P mass\")\n",
        "            watermark = gr.Checkbox(value=False, label=\"Text watermarking\")\n",
        "\n",
        "    model.change(\n",
        "        lambda value: radio_on_change(\n",
        "            value,\n",
        "            typical_p,\n",
        "            watermark,\n",
        "        ),\n",
        "        inputs=model,\n",
        "        outputs=[\n",
        "            typical_p,\n",
        "            watermark,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    inputs.submit(\n",
        "        predict,\n",
        "        [\n",
        "            model,\n",
        "            inputs,\n",
        "            typical_p,\n",
        "            watermark,\n",
        "            chatbot,\n",
        "            state,\n",
        "        ],\n",
        "        [chatbot, state],\n",
        "    )\n",
        "    b1.click(\n",
        "        predict,\n",
        "        [\n",
        "            model,\n",
        "            inputs,\n",
        "            typical_p,\n",
        "            watermark,\n",
        "            chatbot,\n",
        "            state,\n",
        "        ],\n",
        "        [chatbot, state],\n",
        "    )\n",
        "    b1.click(reset_textbox, [], [inputs])\n",
        "    inputs.submit(reset_textbox, [], [inputs])\n",
        "    demo.queue(concurrency_count=16).launch(debug=True)"
      ]
    }
  ]
}